{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Example: LogisticRegression and Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will train as LogisticRegression model on the iris dataset, convert it to ONNX format, simulate historical treatment data, then estimate the treatment effects using our IV estimation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train and Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, X consists of 4 continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtype: float64 \n",
      "Shape: (150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dtype:\", X.dtype, \"\\nShape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our estimation method relies on binary treatment assignment, so here let's assume that iris classifications of 1 or 2 indicate treatment recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clr = LogisticRegression()\n",
    "clr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.71867222e-01, 2.81327780e-02],\n",
       "       [9.79367780e-01, 2.06322203e-02],\n",
       "       [1.28734891e-03, 9.98712651e-01],\n",
       "       [6.26040586e-05, 9.99937396e-01],\n",
       "       [9.81822099e-01, 1.81779006e-02]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clr.predict_proba(X_test)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Save model\n",
    "with open(f\"models/iris_logreg.pickle\", \"wb\") as f:\n",
    "    pickle.dump(clr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load back in the model and convert to ONNX. This step is necessary because our QPS estimation procedure only takes ONNX models for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open(f\"models/iris_logreg.pickle\", 'rb'))\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mlisne package provides an ONNX conversion wrapper function that requires a dummy input and framework string to process datatypes and run the correct conversion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlisne.helpers import convert_to_onnx\n",
    "\n",
    "X_dummy = X[0,:]\n",
    "filename = \"models/iris_logreg.onnx\"\n",
    "\n",
    "convert_to_onnx(model = model, dummy_input = X_dummy, path = filename, framework = \"sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper function will also pass on any keyword arguments to the downstream conversion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_onnx(model=model, dummy_input=X_dummy, path=filename, framework=\"sklearn\", target_opset=12, \n",
    "                doc_string=\"Sklearn LogisticRegression model trained on iris dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simulate historical treatment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate treatment and non-treatment outcomes for each observation i with the following structural equations:\n",
    "\\begin{align}\n",
    "Y_{0i} &= \\mathcal{N}(2,2)x_{1i} + \\mathcal{N}(3,1)x_{2i} + \\mathcal{N}(0,3) \\\\\n",
    "Y_{1i} &= Y_{0i} + \\mathcal{N}(5,1) + \\mathcal{N}(0,1)\n",
    "\\end{align}\n",
    "\n",
    "Where $x_{1}$ and $x_{2}$ refer to the first and second variables of input X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate coefficients for observations\n",
    "b1 = np.random.normal(2,2,len(X))\n",
    "b2 = np.random.normal(3,1,len(X))\n",
    "e0 = np.random.normal(0,3,len(X))\n",
    "treatment_effect = np.random.normal(5,1, len(X))\n",
    "e1 = np.random.normal(0,1,len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = b1 * X[:,0] + b2 * X[:,1] + e0\n",
    "Y1 = Y0 + treatment_effect + e1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate treatment recommendation probabilities using our trained model. Let's assume that the treatment is assigned when the model recommends assignment 75% of the time (no-defiers assumption)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01931014, 0.02628762, 0.01663484, 0.02643487, 0.01708623])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create treatment recommendation probabilities\n",
    "Z_probs = clr.predict_proba(X)[:,1]\n",
    "Z_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw treatment recommendations\n",
    "recommend_prob = np.random.uniform(size=len(X))\n",
    "Z = (recommend_prob <= Z_probs).astype(int)\n",
    "Z[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realized assignment follows recommended assignment 75% of the time\n",
    "treatment_prob = np.random.uniform(size=len(X))\n",
    "D = []\n",
    "for i in range(len(treatment_prob)):\n",
    "    if Z[i] == 1:\n",
    "        if treatment_prob[i] >= 0.75:\n",
    "            D.append(0)\n",
    "        else:\n",
    "            D.append(1)\n",
    "    else:\n",
    "        D.append(Z[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data = np.stack((Y0, Y1, Z, D), axis=1)\n",
    "sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = np.concatenate((sim_data, X), axis=1)\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "cols = ['Y0', 'Y1', 'Z', 'D', 'X1', 'X2', 'X3', 'X4']\n",
    "df = pd.DataFrame(data = full_data, columns=cols)\n",
    "df['Y'] = df['Y1']\n",
    "df.loc[df['D']==0, 'Y'] = df.loc[df['D']==0, 'Y0']\n",
    "df.to_csv(\"data/iris_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 4.88479231098306\n",
      "ATET: 4.956379374710874\n",
      "LATE: 4.923401340422681\n"
     ]
    }
   ],
   "source": [
    "# Treatment effects \n",
    "ate = (df.Y1 - df.Y0).mean()\n",
    "atet = (df.loc[df['D'] == 1, 'Y1'] - df.loc[df['D'] == 1, 'Y0']).mean()\n",
    "late = (df.loc[(df['D'] == df['Z']), 'Y1'] - df.loc[(df['D'] == df['Z']), 'Y0']).mean()\n",
    "print(f\"ATE: {ate}\")\n",
    "print(f\"ATET: {atet}\")\n",
    "print(f\"LATE: {late}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. QPS Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the simulated historical data into our IVEstimatorDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlisne.dataset import IVEstimatorDataset\n",
    "from mlisne.qps import estimate_qps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be loaded upon initialization with optional variable indices, as well as indiviudally post-initialization. \n",
    "\n",
    "Variables without indices will be inferred to be ordered as follows: [Y, Z, D, X_d, X_c], where X_d is the set of discrete input variables, and X_c is the set of continuous input variables. \n",
    "\n",
    "For this example, our input data is all continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.32271498, 12.29033198, 28.79358942,  5.50291316, 46.33375374])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iv_data = IVEstimatorDataset(data=df.drop(['Y0', 'Y1'], axis=1), Z=0, D=1, X_c = range(2,6))\n",
    "iv_data.Y[:5] # Y inferred as last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.32271498, 12.29033198, 28.79358942,  5.50291316, 46.33375374])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overwrite data post-initialization\n",
    "iv_data.load_data(Y = df['Y'], D = df['D'])\n",
    "iv_data.Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(150, 0), dtype=float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discrete data is empty\n",
    "iv_data.X_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now estimate QPS using our IV dataset and converted ONNX model. \n",
    "\n",
    "**Important:** to allow for cross-framework compatibility, our qps estimation function only works with ONNX models that take single inputs or separate continuous/discrete inputs with specified node names, and outputs with name \"output_probability\". The convert_to_onnx function will output with those settings automatically, but please keep this in mind if you plan on using an externally generated ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-a8f277ae2208>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mqps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_qps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miv_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mML_onnx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mqps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\tobin\\mlisne\\mlisne\\qps.py\u001b[0m in \u001b[0;36mestimate_qps\u001b[1;34m(X, S, delta, ML_onnx, seed, types, input_type, input_names)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mQPS_vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\tobin\\mlisne\\mlisne\\qps.py\u001b[0m in \u001b[0;36m_computeQPS\u001b[1;34m(X_ci, types, S, delta, mu, sigma, sess, input_type, input_names, X_di)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;31m# If input type = 1, then coerce all to the continuous type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestandard_draws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_d_long\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcts_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mml_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# Account for case in which output probabilities are in dictionary of class labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'order' is not defined"
     ]
    }
   ],
   "source": [
    "qps = estimate_qps(iv_data, S=100, delta=0.8, ML_onnx=filename)\n",
    "qps[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The converted ONNX model sets expected input dtypes according to the dummy input, but you can also pass the continuous/discrete datatypes explicitly as a tuple to coerce the input data to a specific type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_data.X_c.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qps = estimate_qps(iv_data, S=100, delta=0.8, ML_onnx=filename, types=(np.float64, np.float64))\n",
    "qps[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Treatment effect estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now estimate LATE using our IV approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlisne.estimator import TreatmentIVEstimator\n",
    "\n",
    "est = TreatmentIVEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fit(iv_data, qps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0, b1, b2 = est.coef\n",
    "print(\"Treatment effect estimate:\", b1)\n",
    "print(\"True LATE:\", late)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted estimator also provides other estimation diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First stage details\n",
    "fs = est.firststage\n",
    "\n",
    "print(\"First stage coefficients:\", fs['coef'])\n",
    "print(\"First stage p-values:\", fs['p'])\n",
    "print(\"First stage r2:\", fs['r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.firststage_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
