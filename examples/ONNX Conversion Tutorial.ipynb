{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates the usage of the `convert_to_onnx` helper function for converting a model from any supported framework into ONNX for QPS inference. The function simply wraps the individual conversion functions from [ONNXMLTools](https://github.com/onnx/onnxmltools). This notebook provides examples for conversion from PyTorch, Sklearn, Keras, Tensorflow, and LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as rt\n",
    "import pickle\n",
    "\n",
    "from mlisne import convert_to_onnx\n",
    "from mlisne import estimate_qps_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general all models can be converted with the same basic function call, as demonstrated by the minimum examples below. \n",
    "\n",
    "All that is typically needed for conversion is a model, the framework string, and dummy input data. \n",
    "\n",
    "Aside from PyTorch, conversion from all other frameworks will return the converted model, which can then be sent directly into QPS estimation or an InferenceSession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic examples: Sklearn, Keras, LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on the dummy input data: the data can be in any format or shape as long as it is exactly how you would pass it into a model for inference. \n",
    "\n",
    "`convert_to_onnx` allows for models that take continuous and discrete inputs as separate arrays -- please refer to the Pytorch section for an in-depth example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00224272 0.00168826 0.00191517 0.08352797 0.0016868 ]\n"
     ]
    }
   ],
   "source": [
    "# Minimal example with Sklearn iris \n",
    "data = pd.read_csv(f\"data/iris_data.csv\")\n",
    "data = data.iloc[:,1:]\n",
    "model = pickle.load(open(f\"models/iris_logreg.pickle\", 'rb'))\n",
    "onnx = convert_to_onnx(model, \"sklearn\", data)\n",
    "# We can save the ONNX model to file as well\n",
    "onnx = convert_to_onnx(model, \"sklearn\", data, path = \"../tests/test_models/iris_test.onnx\") \n",
    "\n",
    "# We can estimate QPS/run inference directly with the converted model\n",
    "qps_skl = estimate_qps_onnx(onnx.SerializeToString(), X_c = data)\n",
    "print(qps_skl[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default input name: c_inputs\n",
      "Manual input name: new_name\n"
     ]
    }
   ],
   "source": [
    "# We can also specify a label for the ONNX input node\n",
    "convert_to_onnx(model, \"sklearn\", data, path = \"../tests/test_models/iris_test_named_node.onnx\", input_names = ('new_name',))\n",
    "\n",
    "sess1 = rt.InferenceSession(\"../tests/test_models/iris_test.onnx\")\n",
    "print(\"Default input name:\", sess1.get_inputs()[0].name)\n",
    "\n",
    "sess2 = rt.InferenceSession(\"../tests/test_models/iris_test_named_node.onnx\")\n",
    "print(\"Manual input name:\", sess2.get_inputs()[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX operator number change on the optimization: 10 -> 6\n",
      "The maximum opset needed by this model is only 9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 83\n",
      "Trainable params: 83\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "ir_version: 4\n",
      "producer_name: \"keras2onnx\"\n",
      "producer_version: \"1.7.0\"\n",
      "domain: \"onnxmltools\"\n",
      "model_version: 0\n",
      "doc_string: \"\"\n",
      "graph {\n",
      "  node {\n",
      "    input: \"dense_1_input\"\n",
      "    input: \"dense_1_3/kernel:0\"\n",
      "    output: \"dense_10\"\n",
      "    name: \"dense_1\"\n",
      "    op_type: \"MatMul\"\n",
      "    doc_string: \"\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"dense_10\"\n",
      "    input: \"dense_1_3/bias:0\"\n",
      "    output: \"biased_tensor_name1\"\n",
      "    name: \"Add1\"\n",
      "    op_type: \"Add\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"biased_tensor_name1\"\n",
      "    output: \"dense_1_3/Relu:0\"\n",
      "    name: \"Relu\"\n",
      "    op_type: \"Relu\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"dense_1_3/Relu:0\"\n",
      "    input: \"dense_2_3/kernel:0\"\n",
      "    output: \"dense_20\"\n",
      "    name: \"dense_2\"\n",
      "    op_type: \"MatMul\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"dense_20\"\n",
      "    input: \"dense_2_3/bias:0\"\n",
      "    output: \"biased_tensor_name\"\n",
      "    name: \"Add\"\n",
      "    op_type: \"Add\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"biased_tensor_name\"\n",
      "    output: \"dense_2\"\n",
      "    name: \"Softmax\"\n",
      "    op_type: \"Softmax\"\n",
      "    attribute {\n",
      "      name: \"axis\"\n",
      "      i: -1\n",
      "      type: INT\n",
      "    }\n",
      "    doc_string: \"\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  name: \"sequential_1\"\n",
      "  initializer {\n",
      "    dims: 10\n",
      "    dims: 3\n",
      "    data_type: 1\n",
      "    float_data: 0.2938280701637268\n",
      "    float_data: 1.5117894411087036\n",
      "    float_data: 1.301153302192688\n",
      "    float_data: 5.136831283569336\n",
      "    float_data: 5.088342666625977\n",
      "    float_data: 6.197439193725586\n",
      "    float_data: 0.044205985963344574\n",
      "    float_data: 0.6089378595352173\n",
      "    float_data: 0.8469852209091187\n",
      "    float_data: 0.17862343788146973\n",
      "    float_data: -0.3797987699508667\n",
      "    float_data: -0.6275826692581177\n",
      "    float_data: 5.357481479644775\n",
      "    float_data: 5.714192867279053\n",
      "    float_data: 5.462770462036133\n",
      "    float_data: -0.3656217157840729\n",
      "    float_data: 0.4878771901130676\n",
      "    float_data: -0.3317221403121948\n",
      "    float_data: 3.610504150390625\n",
      "    float_data: 3.977809190750122\n",
      "    float_data: 3.3524036407470703\n",
      "    float_data: 0.4723048210144043\n",
      "    float_data: -0.3717428743839264\n",
      "    float_data: 0.015568554401397705\n",
      "    float_data: 0.21000587940216064\n",
      "    float_data: -0.3780304193496704\n",
      "    float_data: 0.7465175986289978\n",
      "    float_data: 0.2633802890777588\n",
      "    float_data: 0.16934818029403687\n",
      "    float_data: -0.601990818977356\n",
      "    name: \"dense_2_3/kernel:0\"\n",
      "  }\n",
      "  initializer {\n",
      "    dims: 3\n",
      "    data_type: 1\n",
      "    float_data: 0.20922702550888062\n",
      "    float_data: 0.2193034440279007\n",
      "    float_data: 0.2714695334434509\n",
      "    name: \"dense_2_3/bias:0\"\n",
      "  }\n",
      "  initializer {\n",
      "    dims: 4\n",
      "    dims: 10\n",
      "    data_type: 1\n",
      "    float_data: 1.403967022895813\n",
      "    float_data: 7.396064281463623\n",
      "    float_data: 0.7490813136100769\n",
      "    float_data: -0.11311203241348267\n",
      "    float_data: 6.985565662384033\n",
      "    float_data: 0.3099971413612366\n",
      "    float_data: 4.962944030761719\n",
      "    float_data: -0.4338662624359131\n",
      "    float_data: -0.19727672636508942\n",
      "    float_data: -0.4619075655937195\n",
      "    float_data: 0.6736120581626892\n",
      "    float_data: 3.789710283279419\n",
      "    float_data: 0.3835712671279907\n",
      "    float_data: -0.3297440707683563\n",
      "    float_data: 3.8606414794921875\n",
      "    float_data: -0.6357419490814209\n",
      "    float_data: 1.7892625331878662\n",
      "    float_data: -0.29528844356536865\n",
      "    float_data: 0.37374523282051086\n",
      "    float_data: -0.07914149761199951\n",
      "    float_data: 0.8320314884185791\n",
      "    float_data: 4.175485134124756\n",
      "    float_data: 0.384019672870636\n",
      "    float_data: -0.31173568964004517\n",
      "    float_data: 4.727912902832031\n",
      "    float_data: -0.0005796211771667004\n",
      "    float_data: 3.0348520278930664\n",
      "    float_data: -0.06205272674560547\n",
      "    float_data: 0.6679432988166809\n",
      "    float_data: -0.08246040344238281\n",
      "    float_data: 0.43885132670402527\n",
      "    float_data: 1.1326488256454468\n",
      "    float_data: -0.07214339822530746\n",
      "    float_data: -0.2516232132911682\n",
      "    float_data: 1.5277776718139648\n",
      "    float_data: -0.26273566484451294\n",
      "    float_data: 1.2904518842697144\n",
      "    float_data: 0.4371525049209595\n",
      "    float_data: 0.2356555312871933\n",
      "    float_data: -0.5439807176589966\n",
      "    name: \"dense_1_3/kernel:0\"\n",
      "  }\n",
      "  initializer {\n",
      "    dims: 10\n",
      "    data_type: 1\n",
      "    float_data: 0.215406134724617\n",
      "    float_data: 1.1661919355392456\n",
      "    float_data: 0.09815250337123871\n",
      "    float_data: 0.0\n",
      "    float_data: 1.1056692600250244\n",
      "    float_data: -0.013181677088141441\n",
      "    float_data: 0.7666188478469849\n",
      "    float_data: 0.0\n",
      "    float_data: 0.03299902006983757\n",
      "    float_data: 0.0\n",
      "    name: \"dense_1_3/bias:0\"\n",
      "  }\n",
      "  input {\n",
      "    name: \"dense_1_input\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_param: \"N\"\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 4\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output {\n",
      "    name: \"dense_2\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_param: \"N\"\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 3\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"\"\n",
      "  version: 9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Minimal example with Keras iris\n",
    "import keras\n",
    "\n",
    "model = keras.models.load_model(f\"models/keras_example\")\n",
    "print(model.summary())\n",
    "onnx = convert_to_onnx(model, \"keras\", data)\n",
    "print(onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6600082 ],\n",
       "       [0.45872724],\n",
       "       [0.3988191 ],\n",
       "       [0.49400043],\n",
       "       [0.3441565 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimal example with LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "data = pd.read_csv(f\"data/lgbm_regression.test\", header=None, sep='\\t')\n",
    "data = data.drop(0, axis = 1)\n",
    "model = lgb.Booster(model_file= f\"models/lgbm_example.txt\")\n",
    "onnx = convert_to_onnx(model, \"lightgbm\", data)\n",
    "\n",
    "# QPS estimation -- note that the return array is 2D for LightGBM \n",
    "estimate_qps_onnx(onnx.SerializeToString(), X_c = data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM models will return predictions in a 2D array when converted to ONNX. Otherwise the outputs will be the same as inference with the original model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "sess = rt.InferenceSession(onnx.SerializeToString())\n",
    "label_name = sess.get_outputs()[0].name\n",
    "input_name = sess.get_inputs()[0].name\n",
    "\n",
    "onnx_out = sess.run([label_name], {input_name: np.array(data)})[0]\n",
    "print(onnx_out.shape)\n",
    "\n",
    "og_out = model.predict(data)\n",
    "print(og_out.shape)\n",
    "\n",
    "np.testing.assert_array_almost_equal(onnx_out.flatten(), og_out, decimal = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow (In progress...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read in a pretrained model with the below framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "        self.m = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        x = self.m(x)\n",
    "\n",
    "        return x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatModel([(3, 2), (2, 1), (2, 1), (2, 1)], 6, 2, [200,100,50], p=0.4)\n",
    "model.load_state_dict(torch.load(f\"models/churn_categorical.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that this model takes two separate inputs, `x_categorical` and `x_numerical` for discrete and continuous variables, respectively. Our conversion function is well equipped to handle such cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in input data \n",
    "data = pd.read_csv(\"data/churn_data.csv\")\n",
    "\n",
    "# Split discrete and continuous data\n",
    "categorical_cols = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n",
    "numerical_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "\n",
    "for category in categorical_cols:\n",
    "    data[category] = data[category].astype('category')\n",
    "cat = [data[c].cat.codes.values for c in categorical_cols]\n",
    "\n",
    "cat_data = np.stack(cat, 1)\n",
    "num_data = np.array(data[numerical_cols])\n",
    "\n",
    "cat_tensor = torch.tensor(cat_data).long()\n",
    "num_tensor = torch.tensor(num_data).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch conversion is slightly different from the other frameworks in that it makes use of a package-specific `export` function. This function will not return the converted onnx model and requires a file path to save to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For PyTorch to ONNX conversion a file path must be given.\n"
     ]
    }
   ],
   "source": [
    "# Without a file path, the conversion will throw an error\n",
    "try:\n",
    "    convert_to_onnx(model, \"pytorch\", cat_data, num_data)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`convert_to_onnx` allows for models with up to two separate inputs. Keep in mind that the order that these inputs are passed should be the same as their order in the model's `forward` function. We can also pass input names that can be referenced during ONNX inferencing, but these are not required.\n",
    "\n",
    "Models are typically sensitive to data types, so it is important to cast the input data to the expected type before passing them into conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "# Dummy data can be in any format, as long as it is convertible to a tensor and multi-dimensional\n",
    "# The below calls will all result in the same outcome\n",
    "ret = convert_to_onnx(model, \"pytorch\", dummy_input1 = cat_data.astype(np.int64), dummy_input2 = num_data.astype(np.float32), \n",
    "                      path = \"../tests/test_models/churn_cat_test.onnx\", input_names=(\"d_inputs\", \"c_inputs\"))\n",
    "ret2 = convert_to_onnx(model, \"pytorch\", dummy_input1 = cat_data[0,np.newaxis].astype(np.int64), dummy_input2 = num_data[0,np.newaxis].astype(np.float32), \n",
    "                       path = \"../tests/test_models/churn_cat_test.onnx\", input_names=(\"d_inputs\", \"c_inputs\"))\n",
    "ret3 = convert_to_onnx(model, \"pytorch\", dummy_input1 = cat_tensor, dummy_input2 = num_tensor, \n",
    "                       path = \"../tests/test_models/churn_cat_test.onnx\", input_names=(\"d_inputs\", \"c_inputs\"))\n",
    "\n",
    "print(ret, ret2, ret3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the name(s) of the output node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat_out'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_onnx(model, \"pytorch\", dummy_input1 = cat_tensor, dummy_input2 = num_tensor, output_names = [\"cat_out\"], \n",
    "                       path = \"../tests/test_models/churn_cat_test.onnx\", input_names=(\"d_inputs\", \"c_inputs\"))\n",
    "\n",
    "sess = rt.InferenceSession(\"../tests/test_models/churn_cat_test.onnx\")\n",
    "sess.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare inference between ONNX and the original model to validate successful conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 5 decimals\n\nMismatched elements: 9785 / 10000 (97.8%)\nMax absolute difference: 0.9963715\nMax relative difference: 13.200315\n x: array([0.51932, 0.21089, 1.     , ..., 0.06185, 0.07258, 0.10373],\n      dtype=float32)\n y: array([0.28603, 0.2202 , 1.     , ..., 0.14621, 0.09255, 0.10823],\n      dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6e07379cea44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                \"d_inputs\": cat_data.astype(np.int64)})[0]\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monnx_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nArrays are not almost equal to 5 decimals\n\nMismatched elements: 9785 / 10000 (97.8%)\nMax absolute difference: 0.9963715\nMax relative difference: 13.200315\n x: array([0.51932, 0.21089, 1.     , ..., 0.06185, 0.07258, 0.10373],\n      dtype=float32)\n y: array([0.28603, 0.2202 , 1.     , ..., 0.14621, 0.09255, 0.10823],\n      dtype=float32)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch_preds = model(cat_tensor, num_tensor).numpy()\n",
    "sess = rt.InferenceSession(\"../tests/test_models/churn_cat_test.onnx\")\n",
    "output_name = sess.get_outputs()[0].name\n",
    "onnx_preds = sess.run([output_name], {\"c_inputs\": num_data.astype(np.float32),\n",
    "                                      \"d_inputs\": cat_data.astype(np.int64)})[0]\n",
    "\n",
    "np.testing.assert_array_almost_equal(torch_preds, onnx_preds, decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run QPS inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Fail",
     "evalue": "[ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running BatchNormalization node. Name:'BatchNormalization_13' Status Message: CUDNN error executing cudnnBatchNormalizationForwardInference( CudnnHandle(), cudnn_batch_norm_mode_, &alpha, &beta, data_desc, x_data, data_desc, y_data, bn_tensor_desc, scale_data, b_data, mean_data, var_data, epsilon_)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFail\u001b[0m                                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a5aeecfabc6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m estimate_qps_onnx(\"../tests/test_models/churn_cat_test.onnx\", X_c = num_data.astype(np.float32), X_d = cat_data.astype(np.int64), \n\u001b[1;32m----> 2\u001b[1;33m                   input_type = 2)\n\u001b[0m",
      "\u001b[1;32md:\\tobin\\mlisne\\mlisne\\qps.py\u001b[0m in \u001b[0;36mestimate_qps_onnx\u001b[1;34m(onnx, X_c, X_d, data, C, D, L, S, delta, seed, types, input_type, input_names, fcn, vectorized, cpu, iobound, parallel, nprocesses, ntasks, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mcpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mqps_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_computeQPS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monnx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmixed_og_inds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmixed_og_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Compute QPS for each individual i\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m         \u001b[0mqps_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqps_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mqps_vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\tobin\\mlisne\\mlisne\\qps.py\u001b[0m in \u001b[0;36m_computeQPS\u001b[1;34m(onnx, X_c, X_d, L_inds, L_vals, types, S, delta, mu, sigma, input_type, input_names, fcn, cpu, parallel, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mdisc_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_d_long\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisc_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mcts_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference_draws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcts_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mml_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_onnx_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcts_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# If input type = 1, then coerce all to the continuous type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\tobin\\mlisne\\mlisne\\helpers.py\u001b[0m in \u001b[0;36mrun_onnx_session\u001b[1;34m(inputs, sess, input_names, label_names, fcn, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mml_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# All outputs are wrapped in a list -- if single label then send to 1d list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\mlisne-skl\\lib\\site-packages\\onnxruntime\\capi\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFail\u001b[0m: [ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running BatchNormalization node. Name:'BatchNormalization_13' Status Message: CUDNN error executing cudnnBatchNormalizationForwardInference( CudnnHandle(), cudnn_batch_norm_mode_, &alpha, &beta, data_desc, x_data, data_desc, y_data, bn_tensor_desc, scale_data, b_data, mean_data, var_data, epsilon_)"
     ]
    }
   ],
   "source": [
    "estimate_qps_onnx(\"../tests/test_models/churn_cat_test.onnx\", X_c = num_data.astype(np.float32), X_d = cat_data.astype(np.int64), \n",
    "                  input_type = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
