{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll demonstrate how to use the `convert_to_onnx` function to convert a pre-trained tensorflow model with either the saved model or from a frozen graph/checkpoints. This tutorial is adapted from: https://github.com/onnx/tensorflow-onnx/blob/master/tutorials/ConvertingSSDMobilenetToONNX.ipynb\n",
    "\n",
    "**Note:** Conversion from Tensorflow requires that the `tf2onnx` package is installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "WORK = os.path.join(ROOT, \"models\")\n",
    "MODEL = \"ssd_mobilenet_v1_coco_2018_01_28\"\n",
    "\n",
    "# force tf2onnx to cpu\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "os.environ['MODEL'] = MODEL\n",
    "os.environ['WORK'] = WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x ssd_mobilenet_v1_coco_2018_01_28/\n",
      "x ssd_mobilenet_v1_coco_2018_01_28/model.ckpt.index\n",
      "x ssd_mobilenet_v1_coco_2018_01_28/checkpoint\n",
      "x ssd_mobilenet_v1_coco_2018_01_28/pipeline.config\n",
      "x ssd_mobilenet_v1_coco_2018_01_28/model.ckpt.data-00000-of-00001\n",
      "x ssd_mobilenet_v1_coco_2018_01_28/model.ckpt.meta\n",
      "x ssd_mobilenet_v1_coco_2018_01_28/saved_model/\n",
      "x ssd_mobilenet_v1_coco_2018_01_28/saved_model/saved_model.pb\n",
      "x ssd_mobilenet_v1_coco_2018_01_28/saved_model/variables/\n",
      "x ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb\n"
     ]
    }
   ],
   "source": [
    "!cd $WORK; wget -q http://download.tensorflow.org/models/object_detection/$MODEL.tar.gz\n",
    "!cd $WORK; tar zxvf $MODEL.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\r\n",
      "  inputs['inputs'] tensor_info:\r\n",
      "      dtype: DT_UINT8\r\n",
      "      shape: (-1, -1, -1, 3)\r\n",
      "      name: image_tensor:0\r\n",
      "The given SavedModel SignatureDef contains the following output(s):\r\n",
      "  outputs['detection_boxes'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (-1, 100, 4)\r\n",
      "      name: detection_boxes:0\r\n",
      "  outputs['detection_classes'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (-1, 100)\r\n",
      "      name: detection_classes:0\r\n",
      "  outputs['detection_scores'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (-1, 100)\r\n",
      "      name: detection_scores:0\r\n",
      "  outputs['num_detections'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (-1)\r\n",
      "      name: num_detections:0\r\n",
      "Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir $WORK/$MODEL/saved_model/ --tag_set serve  --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see that the model comes as a frozen graph 'frozen_inference_graph.pb' and also as a saved_model. Both can be converted to ONNX. We want to show how to convert both formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Tensorflow models to ONNX through the `convert_to_onnx` API involves slightly different inputs than conversion from the other frameworks. Instead of a model object, the `model` parameters requires a filepath to the saved model, either in a SavedModel, checkpoint, or frozen graph (.pb) format. `output_path` is a required input, and the converted ONNX model will be saved to that path. If the input file is not in the SavedModel format, then the tensorflow graph input and output names will need to be passed through `tf_input_names` and `tf_output_names` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlisne import convert_to_onnx\n",
    "import tensorflow as tf\n",
    "#tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `target_opset` is not specified, then the conversion will default to the maximum opset version supported by the user's ONNX version. With this particular SSD model from Tensorflow, however, only conversion to opset versions 10/11 are supported. Read more about versioning here: https://github.com/onnx/onnx/blob/master/docs/Versioning.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest method of conversion is with the SavedModel format, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_onnx(model = f\"models/{MODEL}/saved_model\", framework = \"tensorflow\",\n",
    "                output_path = f\"models/ssd_mobilenet.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below example demonstrates conversion using the saved frozen graph file instead, which requires inputting the model input and output names as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_onnx(model = f\"models/{MODEL}/frozen_inference_graph.pb\", framework = \"tensorflow\",\n",
    "                 output_path = f\"models/ssd_mobilenet.onnx\", tf_input_names = [\"image_tensor:0\"], \n",
    "                 tf_output_names = ['num_detections:0', 'detection_boxes:0', \n",
    "                                    'detection_scores:0','detection_classes:0'],\n",
    "               target_opset = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the conversion was successful by comparing the prediction results for an input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageColor\n",
    "import math\n",
    "\n",
    "img = Image.open(\"data/ssd_image.jpg\")\n",
    "\n",
    "# we want the outputs in this order\n",
    "outputs = [\"num_detections:0\", \"detection_boxes:0\", \"detection_scores:0\", \"detection_classes:0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific model model does not require fixed image dimension so no resize of the image is needed.\n",
    "But we need to reshape the flat array returned by img.getdata() to HWC and than add an additional dimension to make NHWC, aka a batch of images with 1 image in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 383, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "img_data = np.array(img.getdata()).reshape(img.size[1], img.size[0], 3)\n",
    "img_data = np.expand_dims(img_data.astype(np.uint8), axis=0)\n",
    "print(img_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll run the ONNX converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "sess = rt.InferenceSession(\"models/ssd_mobilenet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sess.run(outputs, {\"image_tensor:0\": img_data})\n",
    "onnx_num_detections, onnx_detection_boxes, onnx_detection_scores, onnx_detection_classes = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run inference on the original model and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model = tf.saved_model.load(f\"models/{MODEL}/saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-21d7c1e228be>:3: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    tf.compat.v1.saved_model.load(sess, ['serve'], f\"models/{MODEL}/saved_model\")\n",
    "    num_detections, detection_boxes, detection_scores, detection_classes = sess.run(outputs, {\"image_tensor:0\": img_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(onnx_num_detections, num_detections, decimal=5)\n",
    "np.testing.assert_array_almost_equal(onnx_detection_boxes, detection_boxes, decimal=5)\n",
    "np.testing.assert_array_almost_equal(onnx_detection_scores, detection_scores, decimal=5)\n",
    "np.testing.assert_array_almost_equal(onnx_detection_classes, detection_classes, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
